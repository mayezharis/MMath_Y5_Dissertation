```{r}
knitr::opts_chunk$set(echo = FALSE)
suppressPackageStartupMessages(library("tidyverse"))
suppressPackageStartupMessages(library("dplyr"))
suppressPackageStartupMessages(library("gridExtra"))
suppressPackageStartupMessages(library("visdat"))
suppressPackageStartupMessages(library("countrycode"))
suppressPackageStartupMessages(library("ggplot2"))
suppressPackageStartupMessages(library("knitr"))
suppressPackageStartupMessages(library("nullabor"))
suppressPackageStartupMessages(library("ggthemes"))
suppressPackageStartupMessages(library("broom"))
suppressPackageStartupMessages(library("latex2exp"))
suppressPackageStartupMessages(library("reshape2"))
suppressPackageStartupMessages(library("readxl"))
suppressPackageStartupMessages(library("lubridate"))
```

```{r}
supply_chain_pricing <- read.csv("C:/Users/mayez/OneDrive - University of Edinburgh/Year 5/Dissertation/Code/mmath_dissertation/Shiny App/Datasets/supply_chain_pricing.csv")
summary(supply_chain_pricing)
```

```{r}
air_quality <- read.csv("C:/Users/mayez/OneDrive - University of Edinburgh/Year 5/Dissertation/Code/mmath_dissertation/Shiny App/Datasets/Air_Quality.csv")
summary(air_quality)
```


```{r}
warehouse_data <- read.csv("C:/Users/mayez/OneDrive - University of Edinburgh/Year 5/Dissertation/Code/mmath_dissertation/Shiny App/Datasets/warehouse_data.csv")
summary(warehouse_data)
```

```{r}
jobs_data <- read.csv("C:/Users/mayez/OneDrive - University of Edinburgh/Year 5/Dissertation/Code/mmath_dissertation/Shiny App/Datasets/nyc_jobs.csv")
summary(jobs_data)
```

```{r}
crime_data <- read_excel("C:/Users/mayez/OneDrive - University of Edinburgh/Year 5/Dissertation/Code/mmath_dissertation/Shiny App/Datasets/violent_crime.xlsx")
crime_data <- na.omit(crime_data)
```


```{r}
e_data <- read.csv("C:/Users/mayez/OneDrive - University of Edinburgh/Year 5/Dissertation/Code/mmath_dissertation/Shiny App/Datasets/energydata_complete.csv")
e_data$date <- strptime(e_data$date, format="%Y-%m-%d %H:%M:%S")
e_data <- na.omit(e_data)

e_data$T_overall <- rowMeans(e_data[,c('T1', 'T2', 'T3', 'T4', 'T5', 'T7', 'T8', 'T9')], na.rm=TRUE)
e_data$H_overall <- rowMeans(e_data[,c('RH_1', 'RH_2', 'RH_3', 'RH_4', 'RH_5', 'RH_6', 'RH_7', 'RH_8', 'RH_9')], na.rm=TRUE)

e_data$T_bedroom <- rowMeans(e_data[,c('T8', 'T9')], na.rm=TRUE)
e_data$H_bedroom <- rowMeans(e_data[,c('RH_8', 'RH_9')], na.rm=TRUE)

e_data$T_shared <- rowMeans(e_data[,c('T1', 'T2', 'T4')], na.rm=TRUE)
e_data$H_shared <- rowMeans(e_data[,c('RH_1', 'RH_2', 'RH_4')], na.rm=TRUE)

# e_data.T_diff <- transform(e_data, new.col = abs(T_overall - T_out))
# e_data.H_diff <- transform(e_data, new.col = abs(H_overall - RH_out))
# 
# e_data.total_energy <- transform(e_data, new.col = Appliances + lights)

e_data$T_diff <- abs(e_data$T_overall - e_data$T_out)
e_data$H_diff <- abs(e_data$H_overall - e_data$RH_out)

e_data$total_energy <- e_data$Appliances + e_data$lights



list_of_vars_e <- c("Appliances", "lights", "total_energy",
                    "T_overall", "H_overall", "T_bedroom", 
                    "H_bedroom", "T_shared", "H_shared", 
                    "T_diff", "H_diff")

outliers <- function(x) {
  Q1 <- quantile(x, probs=.1)
  Q3 <- quantile(x, probs=.9)
  iqr = Q3-Q1
 upper_limit = Q3 + (iqr*1.5)
 lower_limit = Q1 - (iqr*1.5)
 
 x > upper_limit | x < lower_limit
}

remove_outliers <- function(df, cols = names(df)) {
  for (col in cols) {
    df <- df[!outliers(df[[col]]),]
  }
  df
}

e_data <- remove_outliers(e_data, list_of_vars_e)


write.csv(e_data, "C:/Users/mayez/OneDrive - University of Edinburgh/Year 5/Dissertation/Code/mmath_dissertation/Shiny App/Datasets/energy_data_cleaned.csv", row.names=FALSE)

e_data_sample <- sample_n(e_data %>% select(all_of(list_of_vars_e)), 500)

# 
# 
# fit <- lm(Total ~ n_tokens_content, data = e_data_sample)
# plot(fit)
```


```{r THIS IS THE ONE}
news_data <- read_csv("C:/Users/mayez/OneDrive - University of Edinburgh/Year 5/Dissertation/Code/mmath_dissertation/Shiny App/Datasets/OnlineNewsPopularity.csv")
news_data <- na.omit(news_data)
summary(news_data)
```

```{r}
news_data <- read.csv("C:/Users/mayez/OneDrive - University of Edinburgh/Year 5/Dissertation/Code/mmath_dissertation/Shiny App/Datasets/OnlineNewsPopularity.csv")

list_of_vars_n <- c("shares", "n_tokens_title", "n_tokens_content", "n_unique_tokens", "num_hrefs", "num_imgs", "num_videos", "average_token_length")



outliers <- function(x) {
  Q1 <- quantile(x, probs=.25)
  Q3 <- quantile(x, probs=.75)
  iqr = Q3-Q1
 upper_limit = Q3 + (iqr*1.5)
 lower_limit = Q1 - (iqr*1.5)
 
 x > upper_limit | x < lower_limit
}

remove_outliers <- function(df, cols = names(df)) {
  for (col in cols) {
    df <- df[!outliers(df[[col]]),]
  }
  df
}

news_data <- remove_outliers(news_data, list_of_vars_n)

# news_data <- remove_outliers_percentile(news_data, list_of_vars)


write.csv(news_data, "C:/Users/mayez/OneDrive - University of Edinburgh/Year 5/Dissertation/Code/mmath_dissertation/Shiny App/Datasets/news_data_cleaned.csv", row.names=FALSE)

```


